---
name: Statistical Power in Hypothesis Testing
tags: statistics, hypothesis-testing, statistical-power
---

## Q: What is **statistical power** in the context of hypothesis testing?

A: **Statistical power** is the probability that a statistical test will correctly detect an effect when one actually exists. In other words, it is the likelihood that the test will reject the null hypothesis (\( H_0 \)) when the alternative hypothesis (\( H_1 \)) is true[1][3][5]. 

- Power is mathematically defined as \( 1 - \beta \), where \( \beta \) is the probability of a *Type II error* (failing to reject a false null hypothesis)[1][5]. 
- High power means you have a good chance of detecting a true effect; low power means you might miss real effects[3][5].
- Power depends on several factors: **sample size**, **effect size**, **significance level (\( \alpha \))**, and **data variability**.
- A power of at least **80%** is commonly considered adequate in most scientific studies, which means there’s a 20% chance of missing a true effect[3][5].

### Explanation for Non-Technical Readers

Think of **statistical power** as a test's ability to "spot the difference" when a real difference or effect exists. If a test has high power, it's more likely to correctly reveal that there actually is something meaningful happening, instead of missing it by mistake.

For example:  
Imagine you want to know if a new medicine actually works better than the current one. If your study has high statistical power, it means you have a much better chance of noticing the medicine's true benefits (if they exist). If the power is low, your study might not be able to tell the difference—even if the new medicine really is better—perhaps because you don't have enough participants, or the improvement is too small to spot.

Having enough power in a study is important so that valuable time and resources aren't wasted, and to ensure reliable results for decision-making
---
